{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jong104b-kr/AIFFEL_quest_cr/blob/master/Exploration/Quest01/250409_ExCR12_project01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDuUgajcNB-F"
      },
      "outputs": [],
      "source": [
        "# 아래의 코드는 Tensorflow와 Tensorflow Datasets를 사용하여 꽃 이미지 분류 모델을\n",
        "# 학습하는 과정을 설명한다.\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqwzcZdvNKS5"
      },
      "outputs": [],
      "source": [
        "# 1. 상수 정의\n",
        "# BATCH_SIZE: 한 번에 처리할 이미지수로, 32로 설정함\n",
        "# IMG_SIZE: 입력 이미지의 크기로, 224x224 픽셀로 설정함\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aePL_3GqNMJb"
      },
      "outputs": [],
      "source": [
        "# 2. 데이터 로드\n",
        "# tfds.load 함수를 사용하여 'tf_flowers' 데이터셋을 로드함\n",
        "# 데이터셋을 훈련,검증,테스트 세트로 분할함\n",
        "# metadata 객체에서 클래스 이름을 추출함\n",
        "(raw_train, raw_val, raw_test), metadata = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "class_names = metadata.features['label'].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZcvJdbHNb83"
      },
      "outputs": [],
      "source": [
        "# 3. 전처리 및 증강\n",
        "# format_image : 이미지를 지정된 크기(224x224)로 리사이즈하고, 픽셀값을 0~1사이 범위로 정규화함\n",
        "def format_image(image, label):\n",
        "    # 이미지를 지정된 크기로 리사이즈하고 정규화함\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "# augment : 랜덤 플립, 밝기 조정, 대비 조정, 채도 조정, 회전, 크롭 등의 다양한 증강을 적용하여\n",
        "# 데이터 다양성을 높임\n",
        "def augment(image, label):\n",
        "    # 다양한 이미지 증강 기법을 적용함(image, label)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "    k = tf.random.uniform([], 0, 4, dtype=tf.int32)\n",
        "    image = tf.image.rot90(image, k)\n",
        "    crop_size = tf.random.uniform([], int(0.8*IMG_SIZE), IMG_SIZE, dtype=tf.int32)\n",
        "    image = tf.image.random_crop(image, [crop_size, crop_size, 3])\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2JEpNZTNe0m"
      },
      "outputs": [],
      "source": [
        "# 4. 데이터 배치 준비\n",
        "# map함수를 사용하여 전처리 및 증강함수를 데이터셋에 적용함\n",
        "# shuffle, batch, prefetch 메소드를 사용하여 데이터를 효율적으로 준비함\n",
        "train_batches = (\n",
        "    raw_train\n",
        "    .map(format_image, tf.data.AUTOTUNE)\n",
        "    .map(augment, tf.data.AUTOTUNE)\n",
        "    .shuffle(1000)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "validation_batches = raw_val.map(format_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_batches       = raw_test.map(format_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMJUS-EuNg9V"
      },
      "outputs": [],
      "source": [
        "# 5. 모델 구성 (Fine‑tuning)\n",
        "# VGG16 모델을 가져와서 상위 레이어를 제거하고, 이를 기반으로 새로운 분류기를 추가\n",
        "# block5_로 시작하는 레이어만 훈련 가능하게 설정하여 Fine-tuning을 수행함\n",
        "base_model = VGG16(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "                   include_top=False, weights='imagenet')\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = layer.name.startswith('block5_')\n",
        "\n",
        "# Sequential 모델을 정의하는 부분, 이 모델은 사전 학습된 VGG16 모델을 기반으로 하여\n",
        "# 새로운 분류 작업을 수행하도록 설계됨\n",
        "# models.Sequential : Keras에서 제공하는 순차적(Sequential)모델, 여러 개의 층을 순차적으로\n",
        "# 쌓아올릴 수 있음.\n",
        "model = models.Sequential([\n",
        "    # base_model : 사전 학습된 VGG16 모델, 이 모델은 ImageNet 데이터셋에서 미리 학습된\n",
        "    # 가중치를 포함하고 있음.\n",
        "    base_model,\n",
        "    # Flatten 레이어 추가, 이전 레이어의 다차원 출력을 1차원 벡터로 변환함, 이는 완전 연결(Dense)\n",
        "    # 레이어에 입력을 제공하기 위해 필요함.\n",
        "    layers.Flatten(),\n",
        "    # 256개의 뉴런을 가진 완전 연결(Dense) 레이어를 추가함, 활성화 함수는 ReLU,음수값을 0으로 만드는 비선형함수\n",
        "    # L2 정규화를 사용하여 과적합을 방지함, 1e-4는 정규화 강도를 나타냄.\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=l2(1e-4)),\n",
        "    # 배치 정규화를 적용하여 학습을 안정화하고 속도를 높일 수 있도록 함, 각 미니 배치에 대해 입력을 정규화함\n",
        "    layers.BatchNormalization(),\n",
        "    # 드랍아웃 레이어 추가, 드랍아웃 비율을 0.5로 설정하여 과적합을 방지함, 이는 학습 중 무작위로 50%의 뉴런을\n",
        "    # 비활성화한다는 의미임.\n",
        "    layers.Dropout(0.5),\n",
        "    # 최종 Dense레이어 추가(출력층), 클래스의 수만큼 뉴런을 가진 출력 레이어를 추가함, 소프트맥스 활성화 함수를\n",
        "    # 사용하여 각 클래스에 대한 확률을 출력함, 이 함수는 출력 벡터의 합이 1이 되도록 정규화함.\n",
        "    layers.Dense(len(class_names), activation='softmax')\n",
        "    # 여기까지 설명한 모델 구조는 사전학습된 VGG16 모델을 기반으로 하여 새로운 분류 작업을 수행하도록 설계됨.\n",
        "    # 각 레이어는 모델의 성능을 최적화하고 과적합을 방지하기 위해 신중하게 채택됨, 이 모델은 꽃 이미지 분류를 위해 사용함.\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeZRMkN9NjE9"
      },
      "outputs": [],
      "source": [
        "# 6. 컴파일 및 학습\n",
        "# model.compile은 모델을 컴파일하는 메소드, 이 메소드는 모델의 학습을 준비함.\n",
        "model.compile(\n",
        "    # 옵티마이저로 Adam을 사용하고 학습률(learning_rate)을 1e-5로 설정함.\n",
        "    # Adam은 적응형 모멘텀 추정(Adaptive Moment Estimation)을 사용하는 경사하강법의 변형으로\n",
        "    # 학습률을 효과적으로 조절할 수 있음.\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    # 손실함수로 sparse_categorical_crossentropy를 사용함, 이 함수는 정수 레이블을 직접 사용할 때\n",
        "    # 적합하며 모델의 예측과 실제 레이블간 차이를 계산함\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    # 모델 성능을 평가하기 위한 지표로 정확도(Accuracy)를 사용함, 이는 예측이 올바른 비율을 나타냄.\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# model.fit은 모델을 학습시키는 메소드\n",
        "history = model.fit(\n",
        "    # 학습에 사용할 데이터 배치를 포함하는 객체, 이전에 준비한 데이터셋을 배치 단위로 나누어 제공함.\n",
        "    train_batches,\n",
        "    # 총 15번의 에포크(epoch)동안 학습을 진행함, 에포크는 전체 데이터셋을 한 번 완전히 학습하는 것을 의미함.\n",
        "    epochs=15,\n",
        "    # 검증 데이터를 제공하여 각 에포크마다 모델의 성능을 검증함, 이는 과적합을 방지하고 학습 상태를 모니터링 하는데\n",
        "    # 도움을 줌.\n",
        "    validation_data=validation_batches\n",
        ")\n",
        "\n",
        "# 위 과정을 통해 모델은 주어진 데이터셋에서 패턴을 학습하고, 새로운 데이터에 대한 예측능력을 향상시킴\n",
        "# 학습이 완료되면, 모델의 성능을 테스트 세트로 평가하여 최종적인 성능을 확인할 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCfT591YNl5D"
      },
      "outputs": [],
      "source": [
        "# 7. 테스트 세트 성능 평가\n",
        "# 학습된 모델의 성능을 테스트세트에서 평가하는 과정\n",
        "# model_evaluate : 모델의 성능을 평가하는 메소드, test_batches : 테스트 데이터를\n",
        "# 포함하는 객체, 모델 학습 중에 사용되지 않았으며 모델의 일반화 성능을 평가하는데\n",
        "# 사용함, test_loss, test_acc : 메소드가 반환하는 값들, test_loss : 테스트 데이터에\n",
        "# 대한 평균손실값, test_acc : 테스트 데이터에 대한 정확도\n",
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "# test_loss값을 소수점 네자리까지 출력한다\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "# test_acc값을 백분율로 변환하여 소수점 두자리까지 출력함\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "# 위의 과정을 통해 모델의 최종 성능을 확인할 수 있음, 테스트 손실값은 모델이 얼마나\n",
        "# 잘 일반화되었는지 나타내며 테스트 정확도는 모델이 테스트 데이터에서 얼마나 정확하게\n",
        "# 예측했는지를 보여준다. 일반적으로 테스트 정확도가 높을수록 모델이 새로운 데이터에 대해\n",
        "# 잘 일반화된 것으로 간주함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMwoAF-iOQ14"
      },
      "outputs": [],
      "source": [
        "# 8. 결과 시각화\n",
        "# 아래 코드는 matplotlib 라이브러리를 사용하여 딥러닝 모델의 학습 과정 중\n",
        "# 정확도와 손실을 시각화하는 코드임\n",
        "\n",
        "# pyplot모듈을 plt로 임포트하여 그래프를 그릴 수 있게 함\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# epoch의 수를 정의함, 이는 히스토리 객체의 'accuracy'키 만큼 반복됨\n",
        "epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "\n",
        "# 플롯의 크기를 설정함, 하나의 서브플롯을 생성하며 figsize를 적절하게\n",
        "# 조절할 필요가 있음.\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# 정확도 (왼쪽 y축)\n",
        "\n",
        "# 첫 번째 plot함수는 훈련(train)데이터의 정확도를 파란색 선과 원형 마커로 그린다.\n",
        "ax1.plot(epochs, history.history['accuracy'], label='Train Accuracy', color='blue', marker='o')\n",
        "# 두 번째 plot함수는 검증(validation)데이터의 정확도를 청록색 선과 원형 마커로 그린다.\n",
        "ax1.plot(epochs, history.history['val_accuracy'], label='Val Accuracy', color='cyan', marker='o')\n",
        "# set_xlabel, set_ylabel을 통해 x축과 y축의 레이블을 설정함.\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy', color='blue')\n",
        "# tick_params는 y축 눈금의 색상을 지정함.\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# 손실 (오른쪽 y축)\n",
        "# 동일한 x축을 공유하는 두 번째 y축을 추가함.\n",
        "ax2 = ax1.twinx()\n",
        "# 첫 번째 plot함수는 훈련데이터의 손실을 빨간색 점선과 십자가 마커로 그림.\n",
        "ax2.plot(epochs, history.history['loss'], label='Train Loss', color='red', linestyle='--', marker='x')\n",
        "# 두 번째 plot함수는 검증데이터의 손실을 주황색 점선과 십자가 마커로 그림.\n",
        "ax2.plot(epochs, history.history['val_loss'], label='Val Loss', color='orange', linestyle='--', marker='x')\n",
        "# y축 레이블과 색상을 설정함.\n",
        "ax2.set_ylabel('Loss', color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "# 합치기(범례와 제목 설정)\n",
        "# get_legend_handles_labels를 사용하여 각 축의 범례를 가져옴.\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "# 두 범례를 합쳐서 하나의 범례로 만들고 이를 플롯의 오른쪽에 위치함\n",
        "ax1.legend(lines + lines2, labels + labels2, loc='center right')\n",
        "\n",
        "# 플롯의 제목 설정\n",
        "plt.title('Training & Validation Accuracy and Loss')\n",
        "# 그리드를 추가함.\n",
        "plt.grid(True)\n",
        "# 플롯의 레이아웃 조정\n",
        "plt.tight_layout()\n",
        "# 플롯을 표시함\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCk7nA-8QlDt"
      },
      "source": [
        "# <결과 분석>\n",
        "\n",
        "훈련 정확도 (epoch 15): 86.18%\n",
        "\n",
        "검증 정확도 (epoch 15): 86.10%\n",
        "\n",
        "훈련 손실 vs 검증 손실도 꾸준히 같이 떨어짐.\n",
        "\n",
        "## 과적합 거의 없음\n",
        "\n",
        "Test Accuracy: 88.58%\n",
        "\n",
        "Validation Accuracy(최종): 86.10%\n",
        "\n",
        "## 일반적으로 모델이 잘 일반화됨\n",
        "\n",
        "## 강화된 증강과 일부 레이어 미세조정, 규제를 추가\n",
        "## => 과적합이 크게 줄고 테스트 성능도 유의미하게 올랐음\n",
        "\n",
        "### 과적합 감소\n",
        "\n",
        "원래 실험에선 훈련 정확도가 98%까지 올라가고 검증 정확도는 82%에 머물러 두 지표 차이가 컸음\n",
        "\n",
        "개선 실험에선 훈련/검증 정확도가 거의 일치(86.18% vs 86.10%)하며 과적합이 거의 사라졌음\n",
        "\n",
        "### 일반화 성능 향상\n",
        "\n",
        "원래 실험의 Test Acc 83.23% → 개선 후 88.58%로 약 +5.3%p 상승함\n",
        "\n",
        "### 안정적인 수렴\n",
        "\n",
        "원래 실험의 val_loss가 중반 이후 들쑥날쑥했던 반면, 개선 실험에선 val_loss가 점진적으로 안정적으로 감소함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2XwBMiBW8BR"
      },
      "source": [
        "# 이전 모델에서 해바라기를 장미라고 잘못 예측한 예시 1개\n",
        "## 성능 개선한 모델에서 해바라기로 올바르게 예측 성공!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-c459dCWv6E"
      },
      "outputs": [],
      "source": [
        "# files.upload()를 사용하여 사용자가 파일을 업로드 할 수 있도록 함.\n",
        "# 업로드된 파일들은 uploaded라는 딕셔너리에 저장되며 이 딕셔너리의 키는\n",
        "# 파일명이고 값은 업로드된 파일의 바이트 데이터임.\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 이미지 업로드\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 업로드된 파일목록을 반환하며 이를 순회하여 각 파일을 처리함.\n",
        "for fn in uploaded.keys():\n",
        "    # 1) 이미지 로드 및 전처리\n",
        "    # open()으로 파일을 열고, RGB 색상모드로 변환함\n",
        "    img = Image.open(fn).convert('RGB')\n",
        "    # 이미지 크기를 resize함\n",
        "    img_resized = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "    # 픽셀값을 0~1 사이로 정규화함.\n",
        "    x = np.array(img_resized) / 255.0\n",
        "    # 배치 차원을 추가함\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    # 2) 예측\n",
        "    # predict를 통해 모델의 이미지를 입력하여 예측 결과를 얻음.\n",
        "    preds = model.predict(x)[0]  # (1,5) -> (5,)\n",
        "    # np.argmax는 가장 높은 확률을 가진 클래스의 인덱스를 찾음.\n",
        "    top_idx = np.argmax(preds)\n",
        "    # 해당 인덱스에 해당하는 클래스명을 가져옴.\n",
        "    pred_class = class_names[top_idx]\n",
        "    # 해당 클래스의 예측 확률을 가져옴.\n",
        "    pred_prob = preds[top_idx]\n",
        "\n",
        "    # 3) 원본 이미지 표시\n",
        "    # 플롯의 크기를 설정함.\n",
        "    plt.figure(figsize=(4,4))\n",
        "    # imshow()로 이미지를 표시함.\n",
        "    plt.imshow(img)\n",
        "    # 축을 제거함.\n",
        "    plt.axis('off')\n",
        "    # 예측된 클래스명과 확률을 포함한 제목을 설정함.\n",
        "    plt.title(f\"Predicted: {pred_class} ({pred_prob*100:.1f}%)\")\n",
        "    # show()를 호출하여 플롯을 보여줌.\n",
        "    plt.show()\n",
        "\n",
        "    # 4) 클래스별 확률 출력\n",
        "    print(\"=== 클래스별 예측 확률 ===\")\n",
        "    # zip()을 사용하여 클래스명과 예측확률을 묶어 순회함.\n",
        "    for cls, prob in zip(class_names, preds):\n",
        "        print(f\"{cls:10s}: {prob*100:5.1f}%\")\n",
        "\n",
        "# 위의 코드는 사용자가 업로드한 이미지를 전처리하고 사전학습된 모델을 사용하여\n",
        "# 예측한 후 예측 결과를 시각적으로 보여주고 클래스별 예측 확률을 출력함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3z-P2siSEZS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 훈련된 모델을 TFLite 형식으로 변환\n",
        "# Keras모델을 TFLite모델로 변환하기 위한 변환기 객체 생성\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# 변환기를 사용하여 실제 TFLite형식으로 변환된 모델을 생성함\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# TFLite 모델을 파일로 저장\n",
        "# 쓰기전용 바이너리 모드로 파일을 연담, 이 파일에 변환된 TFLite모델을 저장함\n",
        "with open('flower_classifier.tflite', 'wb') as f:\n",
        "    # 변환된 TFLite모델 데이터를 파일에 씀\n",
        "    f.write(tflite_model)\n",
        "# 위 코드는 Keras모델을 TFLite형식으로 변환하여 모델 및 임베디드 장치에서\n",
        "# 실행할 수 있도록 준비하는 과정"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}